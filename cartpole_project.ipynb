{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartpole_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIOf6NCWrZZpP7+iwbJywL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekp2011/cartpole_balance_RL/blob/main/cartpole_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hycmkehVyaq8"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import copy\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import clear_output\n",
        "import math\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.envs.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "IGdGoI2fyhUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_res(values, title=''):   \n",
        "    ''' Plot the reward curve and histogram of results over time.'''\n",
        "    # Update the window after each episode\n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    # Define the figure\n",
        "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
        "    f.suptitle(title)\n",
        "    ax[0].plot(values, label='score per run')\n",
        "    ax[0].axhline(195, c='red',ls='--', label='goal')\n",
        "    ax[0].set_xlabel('Episodes')\n",
        "    ax[0].set_ylabel('Reward')\n",
        "    x = range(len(values))\n",
        "    ax[0].legend()\n",
        "    # Calculate the trend\n",
        "    try:\n",
        "        z = np.polyfit(x, values, 1)\n",
        "        p = np.poly1d(z)\n",
        "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
        "    except:\n",
        "        print('')\n",
        "    \n",
        "    # Plot the histogram of results\n",
        "    ax[1].hist(values[-50:])\n",
        "    ax[1].axvline(195, c='red', label='goal')\n",
        "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
        "    ax[1].set_ylabel('Frequency')\n",
        "    ax[1].legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Krvcx66Fy2yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_search(env, episodes, \n",
        "                  title='Random Strategy'):\n",
        "    \"\"\" Random search strategy implementation.\"\"\"\n",
        "    final = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total = 0\n",
        "        while not done:\n",
        "            # Sample random actions\n",
        "            action = env.action_space.sample()\n",
        "            # Take action and extract results\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            # Update reward\n",
        "            total += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Add to the final reward\n",
        "        final.append(total)\n",
        "        plot_res(final,title)\n",
        "    return final"
      ],
      "metadata": {
        "id": "Ai-y9kSpy-W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get random search results\n",
        "random_s = random_search(env, 150)"
      ],
      "metadata": {
        "id": "p7W4WfB1zCNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Q Learning**"
      ],
      "metadata": {
        "id": "v0LPIEF45hmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN():\n",
        "    ''' Deep Q Neural Network class. '''\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):\n",
        "            self.criterion = torch.nn.MSELoss()\n",
        "            self.model = torch.nn.Sequential(\n",
        "                            torch.nn.Linear(state_dim, hidden_dim),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(hidden_dim, hidden_dim),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(hidden_dim, action_dim)\n",
        "                    )\n",
        "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "\n",
        "\n",
        "\n",
        "    def update(self, state, y):\n",
        "        \"\"\"Update the weights of the network given a training sample. \"\"\"\n",
        "        y_pred = self.model(torch.Tensor(state))\n",
        "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def predict(self, state):\n",
        "        \"\"\" Compute Q values for all actions using the DQL. \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model(torch.Tensor(state))"
      ],
      "metadata": {
        "id": "g55wpuxrzDzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, model, episodes, gamma=0.9, \n",
        "               epsilon=0.3, eps_decay=0.99,\n",
        "               replay=False, replay_size=10, \n",
        "               title = 'DQL', double=False, \n",
        "               n_update=10, soft=False, verbose=True):\n",
        "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
        "    final = []\n",
        "    memory = []\n",
        "    episode_i=0\n",
        "    sum_total_replay_time=0\n",
        "    for episode in range(episodes):\n",
        "        episode_i+=1\n",
        "        if double and not soft:\n",
        "            # Update target network every n_update steps\n",
        "            if episode % n_update == 0:\n",
        "                model.target_update()\n",
        "        # if double and soft:\n",
        "        #     model.target_update()\n",
        "        \n",
        "        # Reset state\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total = 0\n",
        "        \n",
        "        while not done:\n",
        "            # Implement greedy search policy to explore the state space\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                q_values = model.predict(state)\n",
        "                action = torch.argmax(q_values).item()\n",
        "            \n",
        "            # Take action and add reward to total\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # Update total and memory\n",
        "            total += reward\n",
        "            memory.append((state, action, next_state, reward, done))\n",
        "            q_values = model.predict(state).tolist()\n",
        "             \n",
        "            if done:\n",
        "                if not replay:\n",
        "                    q_values[action] = reward\n",
        "                    # Update network weights\n",
        "                    model.update(state, q_values)\n",
        "                break\n",
        "\n",
        "            if replay:\n",
        "                t0=time.time()\n",
        "                # Update network weights using replay memory\n",
        "                model.replay(memory, replay_size, gamma)\n",
        "                t1=time.time()\n",
        "                sum_total_replay_time+=(t1-t0)\n",
        "            else: \n",
        "                # Update network weights using the last step only\n",
        "                q_values_next = model.predict(next_state)\n",
        "                q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
        "                model.update(state, q_values)\n",
        "\n",
        "            state = next_state\n",
        "        \n",
        "        # Update epsilon\n",
        "        epsilon = max(epsilon * eps_decay, 0.01)\n",
        "        final.append(total)\n",
        "        plot_res(final, title)\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"episode: {}, total reward: {}\".format(episode_i, total))\n",
        "            if replay:\n",
        "                print(\"Average replay time:\", sum_total_replay_time/episode_i)\n",
        "    # if replay:\n",
        "    #   model.test()\n",
        "    return final"
      ],
      "metadata": {
        "id": "5pXUj_-xzlHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of states\n",
        "n_state = env.observation_space.shape[0]\n",
        "print(n_state)\n",
        "# Number of actions\n",
        "n_action = env.action_space.n\n",
        "# Number of episodes\n",
        "episodes = 200\n",
        "# Number of hidden nodes in the DQN\n",
        "n_hidden = 50\n",
        "# Learning rate\n",
        "lr = 0.001"
      ],
      "metadata": {
        "id": "1raO-RN4zpa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d99e13-8d8a-46c2-d2dc-d62415b6d050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get DQN results\n",
        "simple_dqn = DQN(n_state, n_action, n_hidden, lr)\n",
        "simple = q_learning(env, simple_dqn, episodes=150, gamma=.9, epsilon=0.9)"
      ],
      "metadata": {
        "id": "MnUc_foxzrkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Q learning using Replay**"
      ],
      "metadata": {
        "id": "gW464Hk15qCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand DQL class with a replay function.\n",
        "class DQN_replay(DQN):\n",
        "    #old replay function\n",
        "    def replay(self, memory, size, gamma=0.9):\n",
        "        \"\"\" Add experience replay to the DQN network class. \"\"\"\n",
        "        # Make sure the memory is big enough\n",
        "        if len(memory) >= size:\n",
        "            states = []\n",
        "            targets = []\n",
        "            # Sample a batch of experiences from the agent's memory\n",
        "            batch = random.sample(memory, size)\n",
        "            \n",
        "            # Extract information from the data\n",
        "            for state, action, next_state, reward, done in batch:\n",
        "                states.append(state)\n",
        "                # Predict q_values\n",
        "                q_values = self.predict(state).tolist()\n",
        "                if done:\n",
        "                    q_values[action] = reward\n",
        "                else:\n",
        "                    q_values_next = self.predict(next_state)\n",
        "                    q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
        "     \n",
        "                targets.append(q_values)\n",
        "\n",
        "            self.update(states, targets)\n",
        "    def test(self):\n",
        "      # torch.save(self.model, 'model.pth')\n",
        "      # model = torch.load('model.pth')\n",
        "      env = gym.make('CartPole-v1')\n",
        "      Tests=[]\n",
        "      for i in range(0,10):\n",
        "        observation = env.reset()\n",
        "        for iteration in range(500):\n",
        "          old_observation = observation\n",
        "          q_values = self.predict(observation)\n",
        "          action = np.argmax(q_values).item()\n",
        "          observation, reward, done, info = env.step(action)\n",
        "          env.render()\n",
        "          if done:\n",
        "            Tests.append(iteration+1)\n",
        "            env.reset()\n",
        "            break\n",
        "   "
      ],
      "metadata": {
        "id": "fLxvpCnHztkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get replay results\n",
        "dqn_replay = DQN_replay(n_state, n_action, n_hidden, lr)\n",
        "replay = q_learning(env, dqn_replay, \n",
        "                    episodes=150, gamma=.99, \n",
        "                    epsilon=0.9, replay=True, \n",
        "                    title='DQL with Replay')"
      ],
      "metadata": {
        "id": "CP6SjSzR0fIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Double Q Learning**"
      ],
      "metadata": {
        "id": "vAy274Hp51aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN_double(DQN):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim, lr):\n",
        "        super().__init__(state_dim, action_dim, hidden_dim, lr)\n",
        "        self.target = copy.deepcopy(self.model)\n",
        "        \n",
        "    def target_predict(self, s):\n",
        "        ''' Use target network to make predicitons.'''\n",
        "        with torch.no_grad():\n",
        "            return self.target(torch.Tensor(s))\n",
        "        \n",
        "    def target_update(self):\n",
        "        ''' Update target network with the model weights.'''\n",
        "        self.target.load_state_dict(self.model.state_dict())\n",
        "        \n",
        "    def replay(self, memory, size, gamma=0.9):\n",
        "        ''' Add experience replay to the DQL network class.'''\n",
        "        if len(memory) >= size:\n",
        "            # Sample experiences from the agent's memory\n",
        "            data = random.sample(memory, size)\n",
        "            states = []\n",
        "            targets = []\n",
        "            # Extract datapoints from the data\n",
        "            for state, action, next_state, reward, done in data:\n",
        "                states.append(state)\n",
        "                q_values = self.predict(state).tolist()\n",
        "                if done:\n",
        "                    q_values[action] = reward\n",
        "                else:\n",
        "                    # The only difference between the simple replay is in this line\n",
        "                    # It ensures that next q values are predicted with the target network.\n",
        "                    q_values_next = self.target_predict(next_state)\n",
        "                    q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
        "\n",
        "                targets.append(q_values)\n",
        "\n",
        "            self.update(states, targets)\n",
        "\n",
        "    def test(self):\n",
        "      # torch.save(self.model, 'model.pth')\n",
        "      # model = torch.load('model.pth')\n",
        "      env = gym.make('CartPole-v1')\n",
        "      Tests=[]\n",
        "      for i in range(0,10):\n",
        "        observation = env.reset()\n",
        "        for iteration in range(500):\n",
        "          old_observation = observation\n",
        "          q_values = self.predict(observation)\n",
        "          action = np.argmax(q_values).item()\n",
        "          observation, reward, done, info = env.step(action)\n",
        "          env.render()\n",
        "          if done:\n",
        "            Tests.append(iteration+1)\n",
        "            env.reset()\n",
        "            break"
      ],
      "metadata": {
        "id": "JGLu515i07VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get replay results\n",
        "dqn_double = DQN_double(n_state, n_action, n_hidden, lr)\n",
        "double =  q_learning(env, dqn_double, episodes=150, gamma=.9, epsilon=0.9, replay=True, double=True, title='Double DQL with Replay', n_update=10)"
      ],
      "metadata": {
        "id": "m-EMY6Yu2K-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "The implementation of the experience replay and the target network have significantly improved the performance of a Deep Q Learning agent in the Open AI CartPole environment."
      ],
      "metadata": {
        "id": "5CbfV9215-P3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AW0dngM92M8G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}